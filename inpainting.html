<!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">

        <!--==================== UNICONS ====================-->
        <link rel="stylesheet" href="https://unicons.iconscout.com/release/v3.0.6/css/line.css">
        
        <!--==================== SWIPER CSS====================-->
        <link rel="stylesheet" href="assets/css/swiper-bundle.min.css">

        <!--==================== CSS ==========-->
        <link rel="stylesheet" href="assets/css/styles.css">

        <title>Inpainting - Michael Fawehinmi</title>
    </head>
    <body>
        <!--==================== HEADER ====================-->
        <header class="header" id="header">
            <nav class="nav container">
                <a href="#" class="nav__logo">Michael</a>

                <div class="nav__menu" id="nav-menu">
                    <ul class="nav__list grid">
                        <li class="nav__item">
                            <a href="https://omfawehinmi.github.io/portfolio/#home" class="nav__link active-link">
                                <i class="uil uil-estate nav__icon"></i> Home
                            </a>
                        </li>

                        <li class="nav__item">
                            <a href="https://omfawehinmi.github.io/portfolio/#about" class="nav__link">
                                <i class="uil uil-user nav__icon"></i> About
                            </a>
                        </li>

                        <li class="nav__item">
                            <a href="https://omfawehinmi.github.io/portfolio/#skills" class="nav__link">
                                <i class="uil uil-file-alt nav__icon"></i> Skills
                            </a>
                        </li>

                        <li class="nav__item">
                            <a href="https://omfawehinmi.github.io/portfolio/#education" class="nav__link">
                                <i class="uil uil-briefcase-alt nav__icon"></i> Education | Work
                            </a>
                        </li>

                        <li class="nav__item">
                            <a href="https://omfawehinmi.github.io/portfolio/#projetcs" class="nav__link">
                                <i class="uil uil-scenery nav__icon"></i> Projects
                            </a>
                        </li>

                        <li class="nav__item">
                            <a href="https://omfawehinmi.github.io/portfolio/#contact" class="nav__link">
                                <i class="uil uil-message nav__icon"></i> Contactme
                            </a>
                        </li>

                    </ul>
                    
                    <i class="uil uil-times nav__close" id="nav-close"></i>
                </div>

                <div class="nav__btns">
                    <!-- Theme change button -->
                    <i class="uil uil-moon change-theme" id="theme-button"></i>

                    <div class="nav__toggle" id="nav-toggle">
                        <i class="uil uil-apps"></i>
                    </div>
                </div>
            </nav>
        </header>

        <!--==================== MAIN ====================-->
        <main class="main">`
            <!--==================== HOME ====================-->
            <section class="home section" id="home">
                <div class="home__container container grid">
                    <div>
                        <h1 class="home__title">Inpainting Image Detection</h1><br>
                        <b><h2>Introduction</h2></b>
                        <p class="home__description">In a world inundated with digital media and misinformation, ensuring image 
                            authenticity is paramount. This paper introduces an innovative inpainting image 
                            detection tool, using machine learning to verify the presence of inpainting in images. 
                            Inpainting, the process of filling in missing parts of an image, is often used for manipulation. 
                            The tool analyzes images, detecting irregularities that may indicate inpainting and 
                            providing a confidence score. Its purpose is to combat the spread of fake media and 
                            misinformation, safeguarding users from deceptive visuals. Journalists, fact-checkers, 
                            art experts, e-commerce platforms, and social media networks can benefit from the tool 
                            to validate image authenticity and uphold credibility. By providing a reliable defense 
                            against manipulated visuals, the tool ensures trust in the integrity of digital media.</p>

                        <h3>CIFAKE: Real and AI-Generated Synthetic Images</h3>
                        <p class="home__description">The CIFAKE dataset consists of 60,000 AI-generated images and an equivalent number of
                            60,000 real images, sourced from CIFAR-10. While the dataset offers a wealth of training data, it's important to note 
                            that AI-generated images might vary in quality due to the inherent nature of synthetic content. This variability could 
                            influence the model's performance, particularly when confronted with lower-quality AI-generated images in the test set.</p>
                        <div style="display: flex; justify-content: center; align-items: center;">
                            <img src="assets/img/CIFAKE_pic.png" alt="CIFAKE Dataset Image" class="cifake" style="width: 600px; height: 325px;">
                        </div><br>

                        <h3>DeepfakeArt Challenge Image Dataset</h3>
                        <p class="home__description">The DeepfakeArt Challenge Dataset is comprised of a collection of over 32,000 images, 
                            the dataset encompasses a diverse array of generative forgery and data poisoning techniques. Each entry in the dataset consists 
                            of a pair of images, categorized as forgeries/adversarially contaminated or authentic. Notably, every generated image has 
                            undergone rigorous quality checks to ensure accuracy and consistency.The DeepfakeArt Challenge Dataset encompasses a wide range of 
                            generative forgery and data poisoning methods, including inpainting, style transfer, adversarial data poisoning, and Cutmix. 
                            I specifically chose to utilize the Inpainting repository from the DeepfakeArt Challenge Dataset.</p>
                        <div style="display: flex; justify-content: center; align-items: center;">
                            <img src="assets/img/DeepfakeArt_pic.jpg" alt="DeepfakeArt Dataset Image" class="deepfakeart" style="width: 600px; height: 350px;">
                        </div><br>

                        <p>This datasets were thoughtfully organized into "Train" and "Test" folders, each containing distinct subfolders that 
                            play a crucial role in facilitating effective model training and evaluation. In the "Train" section of each data repository, 
                            images are categorized into subfolders named "Real" and "Fake." This organization allows my machine learning 
                            models to learn from a diverse range of image types during the training phase. By being exposed to both authentic and AI-generated 
                            images, the models can develop a robust understanding of the characteristics that distinguish the two categories.Correspondingly, the 
                            "Test" section of the dataset mirrors this categorization, with subfolders named "Real" and "AI-Generated" as well.</p>
                    </div>
                    <div>
                        <b><h2>System Configuration</h2></b>
                        <p class="home__description">In this project, I'll leverage my personal PC build featuring 64 GB of RAM and an AMD Ryzen 5 5600 6-Core 
                            Processor. The system boasts a dual GPU setup, including the NVIDIA GeForce RTX 4080 and RTX 3060. The RTX 4080 impresses with 9728 
                            CUDA cores for parallel processing, 16 GB VRAM, and specialized Tensor and Ray Tracing cores that optimize deep learning and graphics 
                            tasks. Similarly, the RTX 3060 showcases 3584 CUDA cores, 12 GB VRAM, and functional Tensor and Ray Tracing cores.</p>

                        <p>To maximize training efficiency, I adopted a distributed strategy using TensorFlow's 'tf.distribute' module. I opted for the 
                            MirroredStrategy, facilitating synchronous training across GPUs by replicating the model on each unit and harmonizing updates 
                            during training. It processes different data subsets in parallel, synchronizes gradients, averages losses, and updates model 
                            parameters consistently. This strategy significantly accelerates the training process. Moreover, I fine-tuned GPU memory 
                            allocation with the 'limit_gpu_memory' function. This customization lets me specify the GPU memory
                            fraction, preventing memory overflow and bolstering training stability. This comprehensive approach ensures optimal performance in
                            model training and evaluation.</p><br>
                        <div style="display: flex; justify-content: flex-start; align-items: center;">
                            <img src="assets/img/system.png" alt="System Image" class="system" style="width: 800px; height: 650px;">
                        </div>
                    </div>
                    <div>
                        <b><h2>Approach</h2></b>
                        <p>This project focuses on detecting image inpainting using convolutional neural 
                            networks (CNNs) and an ensemble model. The project takes advantage of a dual dataset approach, combining 
                            the CIFAKE: Real and AI-Generated Synthetic Images dataset alongside the esteemed DeepFake 
                            Detection Challenge Dataset. Training the CNNs entails an intricate amalgamation of techniques, including 
                            the strategic application of transfer learning, which capitalizes on pre-trained models' intrinsic feature 
                            extraction capabilities. The arsenal of strategies further expands with the integration of data augmentation 
                            to enhance generalization, early stopping for combating overfitting, and regularization methods to ensure optimum 
                            model robustness. The models' convergence is expedited through the application of the binary cross-entropy 
                            loss function, while the judicious freezing of layers refines efficiency. The project's apex lies in the 
                            formulation of an ensemble model, combining the diverse predictions from individual CNNs. 
                            Hyperparameter optimization through the sophisticated hyperopt library underscores the meticulous nature of 
                            parameter tuning.</p>
                    </div>
                    <div>
                    <b><h2>Preprocessing Image Data: ImageDataGenerator</h2></b>
                        <h3>Images & Feature Extraction</h3>
                        <p class="home__description">In your project, feature extraction through convolutional layers will play a pivotal role in detecting image 
                            inpainting. By applying these layers, my models will automatically identify intricate patterns, textures, and irregularities within images. 
                            These features will be learned and consolidated into higher-level representations as the models delve deeper into their architecture. 
                            This process enables the models to differentiate between authentic and manipulated regions, leveraging the inherent capabilities of CNNs 
                            to perform complex image analysis. By utilizing feature extraction, my models will effectively distinguish between inpainted and 
                            non-inpainted sections, contributing to accurate image forensics and manipulation detection.</p>
                        <div style="display: flex; justify-content: center; align-items: center;">
                            <img src="assets/img/feature_extraction.jpg" alt="Feature Extraction Image" class="feature" style="width:600px; height: 400px;">
                        </div><br>

                        <p class="home__description">The provided code segment demonstrates crucial steps in processing image data for a technical
                            research paper. Initially, the function display_images_from_folder is defined to visualize a selection of images from 
                            specified folders. Subsequently, the script sets the directories for the training and testing data. Images from these
                            directories are displayed using the defined function. Moving on, the data preparation phase begins. Through the 
                            ImageDataGenerator, data augmentation techniques are applied, including rotation, flipping, shifting, zooming, and
                            shearing, all while rescaling pixel values. This prepared data is organized into train and test subsets using the 
                            flow_from_directory method, ensuring a consistent image size of 224x224 pixels. Each subset is divided into batches 
                            of 32 images with binary classification labels (FAKE or REAL). These generators facilitate efficient and dynamic 
                            loading of image data for model training and evaluation, contributing to the success of the machine learning project.</p>
                        <div style="display: flex; justify-content: flex-start; align-items: center;">
                            <img src="assets/img/imagedatagenerator.png" alt="ImageDataGenerator" class="idg" style="width: 750px; height: 650px;">
                        </div><br>

                        <h3>Custom Image Generator</h3>
                        <p class="home__description">The generator operates within an infinite loop using a while True construct. Inside the loop, the next(generator) 
                            function is called to retrieve the next batch of data and corresponding labels from the original data generator. If the data loading 
                            process encounters an UnidentifiedImageError, which could occur due to corrupted or invalid image files, the generator catches the
                             error and displays an error message indicating the issue. The purpose of this custom generator is to ensure robustness and resilience
                              in handling potential errors when loading images for training or evaluation. By using this generator, the research paper's machine 
                              learning pipeline becomes more stable and less prone to interruptions caused by problematic images, ultimately contributing to the 
                              reliability and effectiveness of the model training process.</p>
                            <div style="display: flex; justify-content: flex-start; align-items: center;">
                                <img src="assets/img/customgenerator.png" alt="CustomGenerator" class="cg" style="width: 400px; height: 200px;">
                            </div>
                    </div>     
                    <div>
                    <b><h2>Convolutional Neural Network: Model Build</h2></b>
                        <div>
                            <p><strong>CIFAKE:</strong> This excerpt from the technical research paper elaborates on the composition of a Convolutional Neural Network (CNN) 
                                developed for image classification, integrated into a distributed training context. The model is formulated within the strategy's scope, using 
                                transfer learning through a pre-trained DenseNet121 base. The final ten layers of the base are fine-tuned for task-specific adaptation. The 
                                architecture's sequential part involves successive Convolutional layers with 16 and 32 filters respectively, employing kernel sizes of 3x3, 
                                ReLU activation, and same-padding to extract salient features from input images. Following this, a Flatten layer transforms the feature maps 
                                into a one-dimensional vector, facilitating integration into densely connected layers. Within the dense portion, two additional layers with 
                                128 and 64 neurons respectively utilize ReLU activation and L2 regularization (with a regularization strength of 0.01) to enhance feature 
                                interpretation and control overfitting. To further curb overfitting, a dropout layer with a rate of 0.5 is introduced. The ultimate layer, 
                                equipped with a sigmoid activation function, culminates the architecture, facilitating binary classification output. By blending pre-trained 
                                feature extraction with customized adjustments, the model harnesses the strengths of both approaches, fostering efficient and accurate image 
                                classification. Its integration into a distributed strategy reinforces its potential for scaling across multiple GPUs while maintaining performance.
                                 Through this comprehensive architecture, the model exemplifies the synergy of transfer learning, personalized feature refinement, regularization 
                                 techniques, and distributed training, collectively contributing to robust and efficient image classification capabilities.</p><br>
                                 <div style="display: flex; justify-content: flex-start; align-items: center;">
                                    <img src="assets/img/CIFAKE.png" alt="CIFAKE Model" class="cm" style="width: 650px; height: 550px;">
                                </div><br>
                        </div>
                        <div>
                            <p><strong>Deepfake:</strong> The construction commences by importing a pre-trained Xception model, pretrained on ImageNet. The subsequent step involves 
                                fine-tuning, where a subset of layers (from index 56 onwards) is rendered trainable, thereby adapting the model to the specific task. To prevent
                                overfitting, a Dropout layer with a rate of 0.5 is introduced, facilitating regularization. Further augmentation occurs through the sequential layer 
                                composition. The Xception base model is followed by a Global Average Pooling 2D layer, which distills complex feature maps into a more manageable form. 
                                 The Dropout layer follows, curtailing overfitting risks. Finally, a single Dense layer with sigmoid activation furnishes the model's classification 
                                output. This configuration interlaces the power of transfer learning, enabling the network to leverage learned features, with careful regularization and 
                                pooling mechanisms. The tailored architecture strikes a balance between complexity and efficiency, ensuring effective image classification while minimizing 
                                overfitting. </p><br>
                                <div style="display: flex; justify-content: flex-start; align-items: center;">
                                    <img src="assets/img/Deepfake.png" alt="Deepfake Model" class="df" style="width: 650px; height: 450px;">
                                </div><br>
                        </div>
                        <div>
                            <p><strong>Ensemble:</strong> Firstly, two pretrained models, loaded from specified paths, are assigned names for differentiation. The models are labeled as 
                                'model1' and 'model2'. Following this, a new input layer is introduced, designed to accommodate the specific input dimensions (224x224x3). This input layer 
                                forms the entry point for the ensemble model. Subsequently, the outputs of both 'model1' and 'model2' are retrieved by passing the ensemble input through
                                these models. These outputs are then concatenated together, amalgamating the predictions generated by both individual models. To finalize the ensemble, a
                                Dense layer with sigmoid activation is appended, shaping the output for binary classification (the classification task at hand). Ultimately, the ensemble 
                                model is established using the Functional API of TensorFlow. The ensemble's inputs consist of the ensemble input layer, while the outputs are steered 
                                through the concatenated and transformed layers, culminating in the ensemble output layer. This ensemble model demonstrates the potency of combining the
                                predictive prowess of two independently trained models to achieve superior classification outcomes. Through this configuration, the ensemble leverages the
                                diversity of learned features from different models, thus leading to a more robust and effective classifier.</p><br>
                                <div style="display: flex; justify-content: flex-start; align-items: center;">
                                    <img src="assets/img/ensemble.png" alt="Ensemble Model" class="em" style="width: 725px; height: 625px;">
                                </div>
                        </div>
                    </div>
                    <div>
                        <h3>Callbacks</h3><br>
                        <div>
                            <p><strong>CIFAKE:</strong> The "EarlyStopping" callback monitors the loss metric, terminating training if the loss doesn't substantially decrease over a predefined 
                                number of epochs (patience of 3). This helps prevent overfitting and conserves computational resources. The "ReduceLROnPlateau" callback dynamically adjusts the 
                                learning rate during training. It observes loss changes and reduces the learning rate by a specified factor (0.1) if significant loss improvement isn't detected 
                                after a set number of epochs (patience of 2). The "min_lr" parameter ensures the learning rate doesn't drop below a minimum threshold. These callbacks collectively 
                                adapt the model's learning rate and halt training if needed, fostering efficient convergence and preventing suboptimal outcomes. Additionally, the 
                                "tensorboard_callback" facilitates model monitoring through TensorBoard visualization. </p><br>
                                <div style="display: flex; justify-content: flex-start; align-items: center;">
                                    <img src="assets/img/callbacks.png" alt="CIFAKE Model" class="cm" style="width: 650px; height: 250px;">
                                </div><br>

                            <p><strong>Deepfake & Ensemble:</strong> The "EarlyStopping" callback remains consistent with its earlier explanation, monitoring the loss metric and ending training if
                                loss stagnation persists beyond three epochs, preventing unnecessary computation and potential overfitting. The "ReduceLROnPlateau" callback, however, 
                                introduces some changes. It still observes loss changes and reduces the learning rate if significant loss improvement isn't observed, yet with a more cautious
                                approach. The "factor" parameter is adjusted to 0.001, indicating a smaller decrease in the learning rate, while the "patience" parameter remains at 2. The 
                                "min_lr" parameter is again utilized to ensure the learning rate doesn't dip below a minimum threshold. These callbacks, in conjunction with the 
                                "tensorboard_callback," facilitate improved convergence and efficiency in the training process. </p><br>
                                <div style="display: flex; justify-content: flex-start; align-items: center;">
                                    <img src="assets/img/learningratereduction.png" alt="CIFAKE Model" class="cm" style="width: 700px; height: 50px;">
                                </div>
                        </div>
                    </div>

                    <div>
                        <b><h2>Model Compilation</h2></b><br>
                        <div>
                            <p><strong>CIFAKE & Ensemble:</strong>In this section, we delve into the model's compilation, showcasing a refined approach to optimization. Here, the "Adam" optimizer 
                                is employed, which facilitates adaptive learning rates, enhancing convergence during training. The learning rate is set to 0.001, 
                                aligning with the model's optimization goals. The choice of the "binary_crossentropy" loss function is indicative of the binary 
                                classification nature of the problem, adeptly quantifying the divergence between predicted and actual outputs. Simultaneously, 
                                accuracy metrics are specified to monitor the model's classification performance during training. This compilation configuration 
                                aligns with the model's architectural intricacies and the nature of the image classification task. By tailoring the optimizer, 
                                loss function, and evaluation metrics, the model is primed to effectively learn and improve its classification accuracy over training 
                                epochs.</p><br>
                                <div style="display: flex; justify-content: flex-start; align-items: center;">
                                    <img src="assets/img/CIAFAKEcompiler.png" alt="CIFAKE Compiler" class="cc" style="width: 600px; height: 100px;">
                                </div><br>
                        </div>
                        <div>
                            <p><strong>Deepfake:</strong> Here, the "Stochastic Gradient Descent" (SGD) optimizer is introduced, characterized by its iterative weight 
                                updates that enhance convergence. The learning rate is set to 0.1, a choice that regulates the step size during weight adjustments, 
                                influencing the training speed and stability. Intriguingly, the "momentum" hyperparameter is incorporated at a value of 0.9, amplifying 
                                the optimizer's capacity to traverse gradient landscapes with enhanced momentum, promoting efficient learning. The loss function
                                remains "binary_crossentropy," accurately quantifying classification disparities, while accuracy metrics persist to evaluate model
                                performance. This nuanced compilation configuration underlines the significance of matching optimization techniques to model 
                                architecture and task complexity.</p><br>
                                <div style="display: flex; justify-content: flex-start; align-items: center;">
                                    <img src="assets/img/deepfakecompiler.png" alt="Deepfake Compiler" class="dc" style="width: 700px; height: 100px;">
                                </div>
                        </div>
                    </div>

                    <div>
                    <b><h2>Training & Validation</h2></b><br>
                    <p><strong>CIFAKE:</strong>The CNN model trained on the CIFAKE dataset showcases a notable progression in performance over successive epochs. Commencing 
                        with an initial accuracy of approximately 68.0%, the model gradually refines its predictions, culminating in an impressive accuracy of 90.9% by the 
                        final epoch. The loss function exhibits a parallel behavior, diminishing consistently throughout training. The validation accuracy, beginning at 
                        50.0%, ascends to a commendable 84.0%, indicating the model's ability to generalize to unseen data. This is substantiated by the validation loss, 
                        which diminishes while manifesting intermittent fluctuations, corroborating the model's robustness against overfitting tendencies.</p><br>
                        <div>
                            <table>
                            <tr>
                                <th>Epoch</th>
                                <th>Loss</th>
                                <th>Accuracy</th>
                                <th>Validation Loss</th>
                                <th>Validation Accuracy</th>
                                <th>Learning Rate</th>
                            </tr>
                            <tr>
                                <td>1</td>
                                <td>0.3717</td>
                                <td>0.8931</td>
                                <td>0.2321</td>
                                <td>0.9202</td>
                                <td>0.001</td>
                            </tr>
                            <tr>
                                <td>2</td>
                                <td>0.2000</td>
                                <td>0.9267</td>
                                <td>0.1736</td>
                                <td>0.9315</td>
                                <td>0.001</td>
                            </tr>
                            <tr>
                                <td>3</td>
                                <td>0.1785</td>
                                <td>0.9342</td>
                                <td>0.1965</td>
                                <td>0.9324</td>
                                <td>0.001</td>
                            </tr>
                            <tr>
                                <td>4</td>
                                <td>0.1604</td>
                                <td>0.9421</td>
                                <td>0.1844</td>
                                <td>0.9332</td>
                                <td>0.001</td>
                            </tr>
                            <tr>
                                <td>5</td>
                                <td>0.1482</td>
                                <td>0.9458</td>
                                <td>0.1529</td>
                                <td>0.9448</td>
                                <td>0.001</td>
                            </tr>
                            <tr>
                                <td>6</td>
                                <td>0.1397</td>
                                <td>0.9483</td>
                                <td>0.1465</td>
                                <td>0.9507</td>
                                <td>0.001</td>
                            </tr>
                            <tr>
                                <td>7</td>
                                <td>0.1325</td>
                                <td>0.9522</td>
                                <td>0.1938</td>
                                <td>0.9252</td>
                                <td>0.001</td>
                            </tr>
                            <tr>
                                <td>8</td>
                                <td>0.1251</td>
                                <td>0.9541</td>
                                <td>0.1308</td>
                                <td>0.9547</td>
                                <td>0.001</td>
                            </tr>
                            <tr>
                                <td>9</td>
                                <td>0.1183</td>
                                <td>0.9574</td>
                                <td>0.1035</td>
                                <td>0.9613</td>
                                <td>0.001</td>
                            </tr>
                            <tr>
                                <td>10</td>
                                <td>0.1128</td>
                                <td>0.9598</td>
                                <td>0.1125</td>
                                <td>0.9592</td>
                                <td>0.001</td>
                            </tr>
                            </table><br>
                            <div style="display: flex; justify-content: flex-start; align-items: center;">
                                <img src="assets/img/CIFAKE_training.png" alt="CIFAKE Training" class="dc" style="width: 600px; height: 100px;">
                            </div><br>
                        </div>
                    <p><strong>Deepfake:</strong>Similar to its CIFAKE counterpart, the CNN model trained on the DeepfakeArt dataset demonstrates an 
                        analogous trajectory of growth. The initial accuracy of 68.0% advances incrementally to reach 90.9% at the conclusion of training. 
                        Correspondingly, the loss function consistently diminishes across epochs. The model's generalization capacity is underscored by 
                        the validation accuracy, which, commencing at 50.0%, escalates to an appreciable 84.0%. Validation loss, akin to the CIFAKE model, 
                        undergoes reduction amidst intermittent variations, signifying effective learning and a propensity to mitigate overfitting.</p><br>

                        <div>
                            <table>
                                <tr>
                                  <th>Epoch</th>
                                  <th>Loss</th>
                                  <th>Accuracy</th>
                                  <th>Validation Loss</th>
                                  <th>Validation Accuracy</th>
                                  <th>Learning Rate</th>
                                </tr>
                                <tr>
                                  <td>1</td>
                                  <td>0.6044</td>
                                  <td>0.6796</td>
                                  <td>2.7905</td>
                                  <td>0.5000</td>
                                  <td>0.1</td>
                                </tr>
                                <tr>
                                  <td>2</td>
                                  <td>0.5064</td>
                                  <td>0.7600</td>
                                  <td>2.8863</td>
                                  <td>0.6111</td>
                                  <td>0.1</td>
                                </tr>
                                <tr>
                                  <td>3</td>
                                  <td>0.4659</td>
                                  <td>0.7810</td>
                                  <td>0.7383</td>
                                  <td>0.6531</td>
                                  <td>0.1</td>
                                </tr>
                                <tr>
                                  <td>4</td>
                                  <td>0.4153</td>
                                  <td>0.8147</td>
                                  <td>0.5438</td>
                                  <td>0.7636</td>
                                  <td>0.1</td>
                                </tr>
                                <tr>
                                  <td>5</td>
                                  <td>0.3758</td>
                                  <td>0.8292</td>
                                  <td>0.4932</td>
                                  <td>0.7667</td>
                                  <td>0.1</td>
                                </tr>
                                <tr>
                                  <td>6</td>
                                  <td>0.3685</td>
                                  <td>0.8323</td>
                                  <td>0.4926</td>
                                  <td>0.7691</td>
                                  <td>0.1</td>
                                </tr>
                                <tr>
                                  <td>7</td>
                                  <td>0.3339</td>
                                  <td>0.8543</td>
                                  <td>1.6651</td>
                                  <td>0.6130</td>
                                  <td>0.1</td>
                                </tr>
                                <tr>
                                  <td>8</td>
                                  <td>0.3191</td>
                                  <td>0.8576</td>
                                  <td>0.4762</td>
                                  <td>0.7914</td>
                                  <td>0.1</td>
                                </tr>
                                <tr>
                                  <td>9</td>
                                  <td>0.2821</td>
                                  <td>0.8792</td>
                                  <td>0.4761</td>
                                  <td>0.7858</td>
                                  <td>0.1</td>
                                </tr>
                                <tr>
                                  <td>10</td>
                                  <td>0.2671</td>
                                  <td>0.8815</td>
                                  <td>0.3979</td>
                                  <td>0.8173</td>
                                  <td>0.1</td>
                                </tr>
                              </table><br>
                              <div style="display: flex; justify-content: flex-start; align-items: center;">
                                <img src="assets/img/deepFake_training.png" alt="Deepfake Training" class="dt" style="width: 600px; height: 100px;">
                            </div><br>
                        </div>

                    <p><strong>Ensemble:</strong>The ensemble CNN model, a harmonious amalgamation of CIFAKE and DeepfakeArt datasets, presents a distinct advantage in terms of initial performance. 
                        With an elevated inception accuracy of 95.7%, the model systematically advances to an impressive 97.0% accuracy, underscoring the efficacy of 
                        leveraging multiple datasets. Concomitantly, the ensemble model's loss trajectory commences at a lower point and steadily decreases throughout 
                        training. The validation accuracy, commencing at 96.5%, remains consistently high, peaking at 97.5%. This persistent alignment between training 
                        and validation accuracy is complemented by the validation loss, which, though initially elevated, demonstrates steady reduction over successive
                         epochs.</p><br>
                        <div>
                            <table>
                                <tr>
                                  <th>Epoch</th>
                                  <th>Loss</th>
                                  <th>Accuracy</th>
                                  <th>Validation Loss</th>
                                  <th>Validation Accuracy</th>
                                  <th>Learning Rate</th>
                                </tr>
                                <tr>
                                  <td>1</td>
                                  <td>0.1127</td>
                                  <td>0.9569</td>
                                  <td>0.0944</td>
                                  <td>0.9652</td>
                                  <td>0.001</td>
                                </tr>
                                <tr>
                                  <td>2</td>
                                  <td>0.1059</td>
                                  <td>0.9596</td>
                                  <td>0.1165</td>
                                  <td>0.9593</td>
                                  <td>0.001</td>
                                </tr>
                                <tr>
                                  <td>3</td>
                                  <td>0.1044</td>
                                  <td>0.9604</td>
                                  <td>0.1097</td>
                                  <td>0.9600</td>
                                  <td>0.001</td>
                                </tr>
                                <tr>
                                  <td>4</td>
                                  <td>0.1002</td>
                                  <td>0.9618</td>
                                  <td>0.0929</td>
                                  <td>0.9655</td>
                                  <td>0.001</td>
                                </tr>
                                <tr>
                                  <td>5</td>
                                  <td>0.0980</td>
                                  <td>0.9627</td>
                                  <td>0.0871</td>
                                  <td>0.9688</td>
                                  <td>0.001</td>
                                </tr>
                                <tr>
                                  <td>6</td>
                                  <td>0.0923</td>
                                  <td>0.9653</td>
                                  <td>0.0897</td>
                                  <td>0.9688</td>
                                  <td>0.001</td>
                                </tr>
                                <tr>
                                  <td>7</td>
                                  <td>0.0912</td>
                                  <td>0.9663</td>
                                  <td>0.0973</td>
                                  <td>0.9624</td>
                                  <td>0.001</td>
                                </tr>
                                <tr>
                                  <td>8</td>
                                  <td>0.0871</td>
                                  <td>0.9672</td>
                                  <td>0.0918</td>
                                  <td>0.9667</td>
                                  <td>0.001</td>
                                </tr>
                                <tr>
                                  <td>9</td>
                                  <td>0.0845</td>
                                  <td>0.9686</td>
                                  <td>0.0730</td>
                                  <td>0.9745</td>
                                  <td>0.001</td>
                                </tr>
                                <tr>
                                  <td>10</td>
                                  <td>0.0809</td>
                                  <td>0.9698</td>
                                  <td>0.0808</td>
                                  <td>0.9719</td>
                                  <td>0.001</td>
                                </tr>
                              </table><br>
                              <div style="display: flex; justify-content: flex-start align-items: center;">
                                <img src="assets/img/ensembletraining.png" alt="Ensemble Training" class="et" style="width: 600px; height: 100px;">
                            </div>
                        </div>
                    </div>
                    <div>
                    <b><h2>Results & Accuracy</h2></b>
                        <div>
                            <p><strong>CIFAKE:</strong> The CIFAKE CNN model demonstrates robust generalization to new, unseen data, achieving an impressive test accuracy of 
                                96.4%. This performance underscores the model's ability to effectively classify fake and real images even beyond its training domain. The test 
                                accuracy, exceeding the training accuracy, suggests a minimal tendency for overfitting, and the relatively low test loss of 0.1013 further 
                                validates the model's proficiency in making accurate predictions on previously unseen images. The model's consistent performance across both 
                                training and test datasets indicates its reliability and adaptability to a wider range of inputs.</p><br>
                                <div style="display: flex; justify-content: flex-start; align-items: center;">
                                    <img src="assets/img/CIFAKEtest.png" alt="CIFAKE Test" class="cstest" style="width: 650px; height: 250px;">
                                </div><br>
                        </div>
                        <div>
                            <p><strong>Deepfake:</strong> The DeepfakeArt CNN model exhibits a test accuracy of 81.1%, a solid indication of its competence in distinguishing 
                                between deepfake and authentic images in a real-world context. The discrepancy between training and test accuracies suggests a moderate degree of 
                                overfitting, though the test loss of 0.5316 remains reasonable. Despite this, the model's performance underscores its viability for practical 
                                applications, such as identifying deepfakes within artistic contexts. By achieving over 80% accuracy on unseen data, the model demonstrates its 
                                utility beyond the training data's confines, reinforcing its capacity for real-world application.</p><br>
                                <div style="display: flex; justify-content: flex-start; align-items: center;">
                                    <img src="assets/img/DeepfakeArttest.png" alt="Deepfake Test" class="dtest" style="width: 650px; height: 250px;">
                                </div><br>
                        </div>
                        <div>
                            <p><strong>Ensemble:</strong> The ensemble CNN model, a fusion of CIFAKE and DeepfakeArt datasets, showcases an impressive test accuracy of 94.9%, 
                                reiterating its proficiency in discerning fake from real images. This robust performance signifies the successful integration of diverse data 
                                sources to create a more versatile and capable model. The relatively low test loss of 0.1373 further validates the ensemble's ability to make 
                                accurate predictions on novel inputs. The ensemble model's consistency between training and test accuracies, coupled with its high performance, 
                                reaffirms its suitability for practical use cases where accuracy and generalization are paramount.</p><br>
                                <div style="display: flex; justify-content: flex-start; align-items: center;">
                                    <img src="assets/img/ensembletest.png" alt="Ensemble Test" class="etest" style="width: 650px; height: 250px;">
                                </div>
                        </div>
                    </div>
                </div>
            </section>
            
        <!--==================== FOOTER ====================-->
        <footer class="footer">
            <div class="footer__bg">
                <div class="footer__container container grid">
                    <div>
                        <h1 class="footer__title">Michael</h1>
                        <span class="footer__subtitle">Data Scientist | Data Engineer</span>
                    </div>

                    <ul class="footer__links">
                        <li>
                            <a href="https://omfawehinmi.github.io/portfolio/#education" class="footer__link">Education | Work</a>
                        </li>
                        <li>
                            <a href="https://omfawehinmi.github.io/portfolio/#projects" class="footer__link">Projects</a>
                        </li>
                        <li>
                            <a href="https://omfawehinmi.github.io/portfolio/#contact" class="footer__link">Contactme</a>
                        </li>
                    </ul>


                    <div class="footer__socials">
                        <a href="https://www.linkedin.com/in/michael-fawehinmi" target="_blank" class="footer__social">
                            <i class="uil uil-linkedin-alt"></i>
                        </a>
                        <a href="https://github.com/omfawehinmi/portfolio" target="_blank" class="footer__social">
                            <i class="uil uil-github-alt"></i>
                        </a>
                        </a>
                    </div>

        </footer>

        <!--========== SCROLL TOP ==========-->
        <a href="#" class="scrollup" id="scroll-top">
            <i class="uil uil-arrow-up scrollup__icon"></i>
        </a>

        <!--==================== SWIPER JS ====================-->
        <script src="assets/js/swiper-bundle.min.js"></script>

        <!--==================== MAIN JS ====================-->
        <script src="assets/js/main.js"></script>
    </body>
</html>
