<!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">

        <!--==================== UNICONS ====================-->
        <link rel="stylesheet" href="https://unicons.iconscout.com/release/v3.0.6/css/line.css">
        
        <!--==================== SWIPER CSS====================-->
        <link rel="stylesheet" href="assets/css/swiper-bundle.min.css">

        <!--==================== CSS ==========-->
        <link rel="stylesheet" href="assets/css/styles.css">

        <title>Inpainting - Michael Fawehinmi</title>
    </head>
    <body>
        <!--==================== HEADER ====================-->
        <header class="header" id="header">
            <nav class="nav container">
                <a href="#" class="nav__logo">Michael</a>

                <div class="nav__menu" id="nav-menu">
                    <ul class="nav__list grid">
                        <li class="nav__item">
                            <a href="https://omfawehinmi.github.io/portfolio/#home" class="nav__link active-link">
                                <i class="uil uil-estate nav__icon"></i> Home
                            </a>
                        </li>

                        <li class="nav__item">
                            <a href="https://omfawehinmi.github.io/portfolio/#about" class="nav__link">
                                <i class="uil uil-user nav__icon"></i> About
                            </a>
                        </li>

                        <li class="nav__item">
                            <a href="https://omfawehinmi.github.io/portfolio/#skills" class="nav__link">
                                <i class="uil uil-file-alt nav__icon"></i> Skills
                            </a>
                        </li>

                        <li class="nav__item">
                            <a href="https://omfawehinmi.github.io/portfolio/#education" class="nav__link">
                                <i class="uil uil-briefcase-alt nav__icon"></i> Education | Work
                            </a>
                        </li>

                        <li class="nav__item">
                            <a href="https://omfawehinmi.github.io/portfolio/#portfolio" class="nav__link">
                                <i class="uil uil-scenery nav__icon"></i> Projects
                            </a>
                        </li>

                        <li class="nav__item">
                            <a href="https://omfawehinmi.github.io/portfolio/#contact" class="nav__link">
                                <i class="uil uil-message nav__icon"></i> Contactme
                            </a>
                        </li>

                    </ul>
                    
                    <i class="uil uil-times nav__close" id="nav-close"></i>
                </div>

                <div class="nav__btns">
                    <!-- Theme change button -->
                    <i class="uil uil-moon change-theme" id="theme-button"></i>

                    <div class="nav__toggle" id="nav-toggle">
                        <i class="uil uil-apps"></i>
                    </div>
                </div>
            </nav>
        </header>

        <!--==================== MAIN ====================-->
        <main class="main">`
            <!--==================== HOME ====================-->
            <section class="home section" id="home">
                <div class="home__container container grid">
                    <div>
                        <h1 class="home__title">Inpainting Image Detection</h1><br><br>
                        <b><h2 class="home__subtitle2">Introduction</h2></b>
                        <p class="home__description">In a world inundated with digital media and misinformation, ensuring image 
                            authenticity is paramount. This paper introduces an innovative inpainting image 
                            detection tool, using machine learning to verify the presence of inpainting in images. 
                            Inpainting, the process of filling in missing parts of an image, is often used for manipulation. 
                            The tool analyzes images, detecting irregularities that may indicate inpainting and 
                            providing a confidence score. Its purpose is to combat the spread of fake media and 
                            misinformation, safeguarding users from deceptive visuals. Journalists, fact-checkers, 
                            art experts, e-commerce platforms, and social media networks can benefit from the tool 
                            to validate image authenticity and uphold credibility. By providing a reliable defense 
                            against manipulated visuals, the tool ensures trust in the integrity of digital media.</p>

                        <h3 class="home__subtitle">CIFAKE: Real and AI-Generated Synthetic Images</h3>
                        <p class="home__description">The CIFAKE dataset consists of 60,000 AI-generated images and an equivalent number of
                            60,000 real images, sourced from CIFAR-10. While the dataset offers a wealth of training data, it's important to note 
                            that AI-generated images might vary in quality due to the inherent nature of synthetic content. This variability could 
                            influence the model's performance, particularly when confronted with lower-quality AI-generated images in the test set.</p>
                            <div style="display: flex; justify-content: center; align-items: center;">
                                <img src="assets/img/CIFAKE_pic.png" alt="CIFAKE Dataset Image" class="cifake" style="width: 600px; height: 325px;">
                            </div>
                    </div>

                    <div>
                        <h3 class="home__subtitle">DeepfakeArt Challenge Image Dataset</h3>
                        <p class="home__description">The DeepfakeArt Challenge Dataset is comprised of a collection of over 32,000 images, 
                            the dataset encompasses a diverse array of generative forgery and data poisoning techniques. Each entry in the dataset consists 
                            of a pair of images, categorized as forgeries/adversarially contaminated or authentic. Notably, every generated image has 
                            undergone rigorous quality checks to ensure accuracy and consistency.The DeepfakeArt Challenge Dataset encompasses a wide range of 
                            generative forgery and data poisoning methods, including inpainting, style transfer, adversarial data poisoning, and Cutmix. 
                            I specifically chose to utilize the Inpainting repository from the DeepfakeArt Challenge Dataset.</p>
                            <div style="display: flex; justify-content: center; align-items: center;">
                                <img src="assets/img/DeepfakeArt_pic.jpg" alt="DeepfakeArt Dataset Image" class="deepfakeart" style="width: 600px; height: 375px;">
                            </div>
                    </div>

                    <div>
                        <p class="home__description">This datasets were thoughtfully organized into "Train" and "Test" folders, each containing distinct subfolders that 
                            play a crucial role in facilitating effective model training and evaluation. In the "Train" section of each data repository, 
                            images are categorized into subfolders named "Real" and "Fake." This organization allows my machine learning 
                            models to learn from a diverse range of image types during the training phase. By being exposed to both authentic and AI-generated 
                            images, the models can develop a robust understanding of the characteristics that distinguish the two categories.Correspondingly, the 
                            "Test" section of the dataset mirrors this categorization, with subfolders named "Real" and "AI-Generated" as well.</p>
                    </div>
                    <div>
                        <h3 class="home__subtitle">Images & Feature Extraction</h3>
                        <p class="home__description">In your project, feature extraction through convolutional layers will play a pivotal role in detecting image 
                            inpainting. By applying these layers, my models will automatically identify intricate patterns, textures, and irregularities within images. 
                            These features will be learned and consolidated into higher-level representations as the models delve deeper into their architecture. 
                            This process enables the models to differentiate between authentic and manipulated regions, leveraging the inherent capabilities of CNNs 
                            to perform complex image analysis. By utilizing feature extraction, my models will effectively distinguish between inpainted and 
                            non-inpainted sections, contributing to accurate image forensics and manipulation detection.</p>
                            <div style="display: flex; justify-content: center; align-items: center;">
                                <img src="assets/img/assets/img/feature_extraction.png" alt="Feature Extraction Image" class="feature" style="width:200px; height: 150px;">
                            </div>
                    </div>
                
                    <div>
                        <b><h2 class="home__subtitle2">System Configuration</h2></b>
                        <p class="home__description">In this project, I'll leverage my personal PC build featuring 64 GB of RAM and an AMD Ryzen 5 5600 6-Core 
                            Processor. The system boasts a dual GPU setup, including the NVIDIA GeForce RTX 4080 and RTX 3060. The RTX 4080 impresses with 9728 
                            CUDA cores for parallel processing, 16 GB VRAM, and specialized Tensor and Ray Tracing cores that optimize deep learning and graphics 
                            tasks. Similarly, the RTX 3060 showcases 3584 CUDA cores, 12 GB VRAM, and functional Tensor and Ray Tracing cores.</p><br>
                        <p>To maximize training efficiency, I adopted a distributed strategy using TensorFlow's 'tf.distribute' module. I opted for the 
                            MirroredStrategy, facilitating synchronous training across GPUs by replicating the model on each unit and harmonizing updates 
                            during training. It processes different data subsets in parallel, synchronizes gradients, averages losses, and updates model 
                            parameters consistently. This strategy significantly accelerates the training process.</p>
                        <p>Moreover, I fine-tuned GPU memory allocation with the 'limit_gpu_memory' function. This customization lets me specify the GPU memory
                            fraction, preventing memory overflow and bolstering training stability. This comprehensive approach ensures optimal performance in
                            model training and evaluation.</p>
                            <div style="display: flex; justify-content: center; align-items: center;">
                                <img src="assets/img/system.png" alt="System Image" class="system" style="width: 575px; height: 590px;">
                            </div>
                    </div>

                    <div>
                        <b><h2 class="home__subtitle2">Approach</h2></b>
                        <p class="home__description">This project focuses on detecting image inpainting using convolutional neural 
                            networks (CNNs) and an ensemble model. The project takes advantage of a dual dataset approach, combining 
                            the CIFAKE: Real and AI-Generated Synthetic Images dataset alongside the esteemed DeepFake 
                            Detection Challenge Dataset. Training the CNNs entails an intricate amalgamation of techniques, including 
                            the strategic application of transfer learning, which capitalizes on pre-trained models' intrinsic feature 
                            extraction capabilities. The arsenal of strategies further expands with the integration of data augmentation 
                            to enhance generalization, early stopping for combating overfitting, and regularization methods to ensure optimum 
                            model robustness. The models' convergence is expedited through the application of the binary cross-entropy 
                            loss function, while the judicious freezing of layers refines efficiency. The project's apex lies in the 
                            formulation of an ensemble model, combining the diverse predictions from individual CNNs. 
                            Hyperparameter optimization through the sophisticated hyperopt library underscores the meticulous nature of 
                            parameter tuning.</p>
                    </div>

                    <div>
                    <b><h2 class="home__subtitle2">Preprocessing Image Data: ImageDataGenerator</h2></b>
                        <p class="home__description">The provided code segment demonstrates crucial steps in processing image data for a technical
                             research paper. Initially, the function display_images_from_folder is defined to visualize a selection of images from 
                             specified folders. Subsequently, the script sets the directories for the training and testing data. Images from these
                              directories are displayed using the defined function. Moving on, the data preparation phase begins. Through the 
                              ImageDataGenerator, data augmentation techniques are applied, including rotation, flipping, shifting, zooming, and
                               shearing, all while rescaling pixel values. This prepared data is organized into train and test subsets using the 
                               flow_from_directory method, ensuring a consistent image size of 224x224 pixels. Each subset is divided into batches 
                               of 32 images with binary classification labels (FAKE or REAL). These generators facilitate efficient and dynamic 
                               loading of image data for model training and evaluation, contributing to the success of the machine learning project.</p>
                            <div style="display: flex; justify-content: center; align-items: center;">
                                <img src="assets/img/imagedatagenerator.png" alt="ImageDataGenerator" class="idg" style="width: 475px; height: 490px;">
                            </div>
                    </div>

                    <div>
                        <h3 class="home__subtitle">Custom Image Generator</h3>
                        <p class="home__description">The generator operates within an infinite loop using a while True construct. Inside the loop, the next(generator) 
                            function is called to retrieve the next batch of data and corresponding labels from the original data generator. If the data loading 
                            process encounters an UnidentifiedImageError, which could occur due to corrupted or invalid image files, the generator catches the
                             error and displays an error message indicating the issue. The purpose of this custom generator is to ensure robustness and resilience
                              in handling potential errors when loading images for training or evaluation. By using this generator, the research paper's machine 
                              learning pipeline becomes more stable and less prone to interruptions caused by problematic images, ultimately contributing to the 
                              reliability and effectiveness of the model training process.</p>
                            <div style="display: flex; justify-content: center; align-items: center;">
                                <img src="assets/img/customgenerator.png" alt="CustomGenerator" class="cg" style="width: 325px; height: 275px;">
                            </div>
                    </div>     

                    <div>
                    <b><h2 class="home__subtitle2">Convolutional Neural Network: Model Build</h2></b>
                        <div>
                            <p><strong>CIFAKE:</strong> This excerpt from the technical research paper elaborates on the composition of a Convolutional Neural Network (CNN) 
                                developed for image classification, integrated into a distributed training context. The model is formulated within the strategy's scope, using 
                                transfer learning through a pre-trained DenseNet121 base. The final ten layers of the base are fine-tuned for task-specific adaptation. The 
                                architecture's sequential part involves successive Convolutional layers with 16 and 32 filters respectively, employing kernel sizes of 3x3, 
                                ReLU activation, and same-padding to extract salient features from input images. Following this, a Flatten layer transforms the feature maps 
                                into a one-dimensional vector, facilitating integration into densely connected layers. Within the dense portion, two additional layers with 
                                128 and 64 neurons respectively utilize ReLU activation and L2 regularization (with a regularization strength of 0.01) to enhance feature 
                                interpretation and control overfitting. To further curb overfitting, a dropout layer with a rate of 0.5 is introduced. The ultimate layer, 
                                equipped with a sigmoid activation function, culminates the architecture, facilitating binary classification output. By blending pre-trained 
                                feature extraction with customized adjustments, the model harnesses the strengths of both approaches, fostering efficient and accurate image 
                                classification. Its integration into a distributed strategy reinforces its potential for scaling across multiple GPUs while maintaining performance.
                                 Through this comprehensive architecture, the model exemplifies the synergy of transfer learning, personalized feature refinement, regularization 
                                 techniques, and distributed training, collectively contributing to robust and efficient image classification capabilities.</p>
                        </div>

                        <div>
                            <p><strong>Deepfake:</strong> The construction commences by importing a pre-trained Xception model, pretrained on ImageNet. The subsequent step involves 
                                fine-tuning, where a subset of layers (from index 56 onwards) is rendered trainable, thereby adapting the model to the specific task. To prevent
                                 overfitting, a Dropout layer with a rate of 0.5 is introduced, facilitating regularization. Further augmentation occurs through the sequential layer 
                                 composition. The Xception base model is followed by a Global Average Pooling 2D layer, which distills complex feature maps into a more manageable form. 
                                 The Dropout layer follows, curtailing overfitting risks. Finally, a single Dense layer with sigmoid activation furnishes the model's classification 
                                 output. This configuration interlaces the power of transfer learning, enabling the network to leverage learned features, with careful regularization and 
                                 pooling mechanisms. The tailored architecture strikes a balance between complexity and efficiency, ensuring effective image classification while minimizing 
                                 overfitting. </p>
                        </div>

                        <div>
                            <p><strong>Deepfake:</strong> Firstly, two pretrained models, loaded from specified paths, are assigned names for differentiation. The models are labeled as 
                                'model1' and 'model2'. Following this, a new input layer is introduced, designed to accommodate the specific input dimensions (224x224x3). This input layer 
                                forms the entry point for the ensemble model. Subsequently, the outputs of both 'model1' and 'model2' are retrieved by passing the ensemble input through
                                 these models. These outputs are then concatenated together, amalgamating the predictions generated by both individual models. To finalize the ensemble, a
                                  Dense layer with sigmoid activation is appended, shaping the output for binary classification (the classification task at hand). Ultimately, the ensemble 
                                  model is established using the Functional API of TensorFlow. The ensemble's inputs consist of the ensemble input layer, while the outputs are steered 
                                  through the concatenated and transformed layers, culminating in the ensemble output layer. This ensemble model demonstrates the potency of combining the
                                   predictive prowess of two independently trained models to achieve superior classification outcomes. Through this configuration, the ensemble leverages the
                                    diversity of learned features from different models, thus leading to a more robust and effective classifier.</p>
                        </div>
                    </div>

                    <div>
                        <h3 class="home__subtitle">Callbacks</h3>
                        <p class="home__description">he model is implemented using transfer learning and a pre-trained convolutional neural 
                            network (CNN) architecture called DenseNet121. Transfer learning allows leveraging the knowledge 
                            learned from a large dataset (ImageNet) to improve performance on </p>
                    </div>

                    <div>
                        <h3 class="home__subtitle">Regularization Methods</h3>
                        <p class="home__description">he model is implemented using transfer learning and a pre-trained convolutional neural 
                            network (CNN) architecture called DenseNet121. Transfer learning allows leveraging the knowledge 
                            learned from a large dataset (ImageNet) to improve performance on </p>
                    </div>

                    <div>
                        <b><h2 class="home__subtitle2">Model Compilation</h2></b>
                        <h3 class="home__subtitle">Learning Rate Optimizer: Adam</h3>
                        <p class="home__description">he model is implemented using transfer learning and a pre-trained convolutional neural 
                            network (CNN) architecture called DenseNet121. Transfer learning allows leveraging the knowledge 
                            learned from a large dataset (ImageNet) to improve performance on </p>
                    </div>

                    <div>
                        <h3 class="home__subtitle">Binary Cross-Entropy</h3>
                        <p class="home__description">he model is implemented using transfer learning and a pre-trained convolutional neural 
                            network (CNN) architecture called DenseNet121. Transfer learning allows leveraging the knowledge 
                            learned from a large dataset (ImageNet) to improve performance on </p>
                    </div>
                    

                    <div>
                    <b><h2 class="home__subtitle2">Training & Validation</h2></b>
                        <p class="home__description">Data scientist and data engineer driving impactful business decisions and 
                            automated process improvements. Advanced knowledge of BI tools, programmatic analysis, and data 
                            system integrations. Excellent communications skills and proven leadership abilities.</p>
                    </div>

                    <div>
                        <h3 class="home__subtitle">Hyperparameter Tuning: Hyperopt</h3>
                        <p class="home__description">Data scientist and data engineer driving impactful business decisions and 
                            automated process improvements. Advanced knowledge of BI tools, programmatic analysis, and data 
                            system integrations. Excellent communications skills and proven leadership abilities.</p>
                    </div>

                    <div>
                    <b><h2 class="home__subtitle2">Results & Accuracy</h2></b>
                        <p class="home__description">Data scientist and data engineer driving impactful business decisions and 
                            automated process improvements. Advanced knowledge of BI tools, programmatic analysis, and data 
                            system integrations. Excellent communications skills and proven leadership abilities.</p>
                    </div>
                    
                </div>
            </section>
            
        <!--==================== FOOTER ====================-->
        <footer class="footer">
            <div class="footer__bg">
                <div class="footer__container container grid">
                    <div>
                        <h1 class="footer__title">Michael</h1>
                        <span class="footer__subtitle">Data Scientist | Data Engineer</span>
                    </div>

                    <ul class="footer__links">
                        <li>
                            <a href="https://omfawehinmi.github.io/portfolio/#education" class="footer__link">Education | Work</a>
                        </li>
                        <li>
                            <a href="https://omfawehinmi.github.io/portfolio/#projects" class="footer__link">Projects</a>
                        </li>
                        <li>
                            <a href="https://omfawehinmi.github.io/portfolio/#contact" class="footer__link">Contactme</a>
                        </li>
                    </ul>


                    <div class="footer__socials">
                        <a href="https://www.linkedin.com/in/michael-fawehinmi" target="_blank" class="footer__social">
                            <i class="uil uil-linkedin-alt"></i>
                        </a>
                        <a href="https://github.com/omfawehinmi/portfolio" target="_blank" class="footer__social">
                            <i class="uil uil-github-alt"></i>
                        </a>
                        </a>
                    </div>

        </footer>

        <!--========== SCROLL TOP ==========-->
        <a href="#" class="scrollup" id="scroll-top">
            <i class="uil uil-arrow-up scrollup__icon"></i>
        </a>

        <!--==================== SWIPER JS ====================-->
        <script src="assets/js/swiper-bundle.min.js"></script>

        <!--==================== MAIN JS ====================-->
        <script src="assets/js/main.js"></script>
    </body>
</html>
